{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3b50a-825c-4115-ad21-5d2cba216bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# NEXT WORD PREDICTOR\n",
    "# =========================\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout,Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# =========================\n",
    "# 1. TEXT DATA (Your story)\n",
    "text_data\n",
    "with open('dataForNextWordPredictor.txt') as f:\n",
    "    text_data=f.read()\n",
    "# =========================\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. DATA PREPROCESSING\n",
    "# =========================\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters but keep spaces and periods\n",
    "    text = re.sub(r'[^a-zA-Z\\s\\.]', '', text)\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Split into sentences\n",
    "    sentences = text.split('.')\n",
    "    # Remove empty sentences\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]\n",
    "    return sentences\n",
    "\n",
    "sentences = preprocess_text(text_data)\n",
    "print(f\"Total sentences: {len(sentences)}\")\n",
    "print(f\"Sample sentence: {sentences[0][:50]}...\")\n",
    "\n",
    "# =========================\n",
    "# 3. TOKENIZATION\n",
    "# =========================\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(f\"Total unique words: {total_words}\")\n",
    "\n",
    "# =========================\n",
    "# 4. CREATE SEQUENCES\n",
    "# =========================\n",
    "input_sequences = []\n",
    "for sentence in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "print(f\"Total sequences created: {len(input_sequences)}\")\n",
    "\n",
    "# =========================\n",
    "# 5. PAD SEQUENCES\n",
    "# =========================\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "print(f\"Maximum sequence length: {max_sequence_len}\")\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "# Split into features and labels\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# =========================\n",
    "# 6. TRAIN-TEST SPLIT\n",
    "# =========================\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "\n",
    "# =========================\n",
    "# 7. BUILD MODEL\n",
    "# =========================\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    \n",
    "    Embedding(total_words, 100, input_length=max_sequence_len-1),\n",
    "    Bidirectional(LSTM(200, dropout=0.2, return_sequences=False)),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# =========================\n",
    "# 8. TRAIN MODEL\n",
    "# =========================\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 9. PLOT TRAINING RESULTS\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 6: Training Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =========================\n",
    "# 10. PREDICTION FUNCTION\n",
    "# =========================\n",
    "\n",
    "def predict_next_words(model, tokenizer, text, num_words=3):\n",
    "    \"\"\"\n",
    "    Predict the next words given a seed text\n",
    "    \"\"\"\n",
    "    for _ in range(num_words):\n",
    "        # Convert text to sequence\n",
    "        token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        \n",
    "        # Get predictions\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "        \n",
    "        # Get the word with highest probability\n",
    "        predicted_index = np.argmax(predicted_probs)\n",
    "        \n",
    "        # Convert index to word\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        \n",
    "        text += \" \" + output_word\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the model\n",
    "test_phrases = [\n",
    "    \"sherlock\",\n",
    "    \n",
    "]\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for phrase in test_phrases:\n",
    "    prediction = predict_next_words(model, tokenizer, phrase, num_words=2)\n",
    "    print(f\"'{phrase}' -> '{prediction}'\")\n",
    "\n",
    "# =========================\n",
    "# 11. INTERACTIVE MODE\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 8: Interactive Predictor\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Type 'quit' to exit\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nEnter a word or phrase: \").lower()\n",
    "    \n",
    "    if user_input == 'quit':\n",
    "        break\n",
    "    \n",
    "    if user_input.strip() == \"\":\n",
    "        continue\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = predict_next_words(model, tokenizer, user_input, num_words=3)\n",
    "    print(f\"Next words: {prediction}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2313d-ab75-42dd-a7de-5e73da25e35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
